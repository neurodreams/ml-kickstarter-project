# Kickstarter Project Success Predictor

This project is a Python-based machine learning project that predicts the probability of success for Kickstarter projects.
The project combines data preprocessing, feature engineering, model evaluation and a Streamlit-based user interface into a single end-to-end application.

---

## Team: CatGPT

This project was developed as a team project as part of ICT engineering studies.

- [Kim Erkkilä](https://github.com/Kimerkki) - Project Owner, Developer  
- Sara Pyky - Project Owner, Developer  
- Valtteri Mikkola - Scrum Master, Developer  
- Niko Kyllönen - Scrum Master, Developer  

---

## Who is this project for?

This project is intended for:  
- Data science and machine learning students  
- Developers interested in end-to-end ML pipelines  
- Anyone curious about what factors influence Kickstarter project success  

---

## What does this project do?

This project allows the user to:  
- Process and enrich raw Kickstarter data  
- Train and evaluate multiple machine learning models  
- Compare model performance  
- Predict project success probability via a Streamlit web application  

---

## Quick start

This project requires the Kickstarter dataset to run.
The Streamlit application depends on a processed dataset used for model evaluation and visualization.  


> Note:
> This project is designed to be fully reproducible.
> For this reason, no datasets are included in the repository and all results
> are generated from the original source data.  

### 1. Clone the repository with SSH

```bash
git clone git@github.com:neurodreams/ml-kickstarter-project.git
```

### 2. Download the dataset

Dataset source (Sample available only):
[original dataset source](https://www.kaggle.com/datasets/domingosun/kickstarter-dataset-629147-projects-may-2025)

Download data (Full version):
[full data](https://edukainuu-my.sharepoint.com/:x:/r/personal/kimerkkila_kamk_fi/Documents/Datoja/Kickstarter%20Projects_629147_Full_May%202025.csv)

Download the dataset, name it "raw_data.csv" and place the CSV file here:
```bash
data/raw_data.csv
```

### 3. Set up the environment

```bash
uv sync
uv pip install -e .
```

### 4. Run data preprocessing

> **Note:** Feature engineering may take **30–60 minutes** depending on your system.

```bash
uv run -m preprocessing.preprocess_data
uv run -m preprocessing.feature_engineering
```  
These steps generate following data files:
```bash
data/processed_data.csv
data/feature_processed_data.csv
```  

### 5. Train and evaluate models (OPTIONAL)

This step is **Optional**, shows and saves the evaluation report from different models.  
Model evaluation scores can be generated by this step:  

```bash
uv run -m models.evaluate_models
```

### 6. Train and save the optimized LogisticRegression model (OPTIONAL)

This step is **Optional**, the final optimized LogisticRegression model is already saved in:  
```bash
src/models/logreg_final.joblib
```

Run the script:  
```bash
uv run -m models.logreg_optimized
```

### 7. Run the Streamlit application
```bash
uv run streamlit run app/kurkkumopo_app.py
```  
The application will open at:
http://localhost:8501

---

## Perplexity API integration (OPTIONAL)

The Streamlit application includes an optional feature that uses the Perplexity API to generate AI-powered improvement suggestions for projects.

- The application works **without** an API key.
- This feature is only required for **Perplexity-generated suggestions**.

### Setting up the API key

The Streamlit application uses Perplexity model: sonar-pro

1. Follow the instructions in perplexity documentation for obtaining an API-key:
    https://docs.perplexity.ai/getting-started/overview  

2. Create a `.env` file in the project root with the API-key:
```env
PERPLEXITY_API_KEY=pplx-************************************************
```  

When the API key is present, the Perplexity features will be enabled automatically in the Streamlit UI.  

---

## Run with Docker (alternative)

The Streamlit app can also be run in Docker container.  

1. Build the container  

```bash
docker build -t kickstarter-app .
```

2. Run the container  

```bash
docker run --env-file .env -p 8501:8501 kickstarter-app
```

---

## Technologies used

- **Python** - Core programming language  
- **uv** - Dependency and virtual environment management  
- **pandas** - Data processing  
- **scikit-learn** - Machine learning models and evaluation  
- **Streamlit** - Interactive web application  
- **Docker** - Containerized deployment  
- **JupyterLab** - Exploratory data analysis  
- **Kaggle dataset** - Data source  
- **Perplexity API** - AI-powered improvement suggestions  

---

## Project structure
  
catgpt/  
├── app/ # Streamlit application  
│ ├── assets/ # Streamlit assets  
│ └── pages/ # Streamlit pages  
│  
├── data/ # Raw and processed datasets (not tracked)  
│ └── fx_rates/ # Fx-rate datasets for correct USD_pledged  
│  
├── playground/ # Jupyter notebooks for testing  
│  
├── src/  
│ ├── preprocessing/ # Data preprocessing and feature engineering  
│ ├── models/ # Model training and evaluation  
│ └── utils/ # Helper functions  
│  
├── tests/ # Alternative folder for testing  
├── .env  
├── .gitignore  
├── .python-version  
├── Dockerfile  
├── pyproject.toml  
├── README.md  
└── uv.lock  

---

## Module usage and development

### Enabling ```src/``` modules  
Modules inside the ```src/``` directory are enabled by installing the project in editable mode:  
```bash
uv pip install -e .
```  
This is a **local environment change only** and affects the active ```.venv```.  

After this, imports work as follows:  
```python
from utils.data_preparation import prepare_data
```  

---

## Data preprocessing

The preprocessing pipeline can be run from the command line:  
```bash
uv run -m preprocessing.preprocess_data
```  
The pipeline:  

- Loads raw data from ```data/raw_data.csv```  
- Cleans and processes the dataset  
- Saves the result as both CSV and Parquet files  
- Prints row counts and dropped rows for transparency  

---

## Feature engineering

Additional features such as:  

- usd_goal_fx_log  
- creator_prev_projects  
- creator_prec_projects_successful  
- project_duration_days  
- category_name_reduced  

are added in the feature engineering step.

Run feature engineering with:  
```bash
uv run -m preprocessing.feature_engineering
```  
This generates ```feature_processed_data.csv``` in the ```data/``` directory.  

---

## Model evaluation

Model comparison and evaluation is performed using the engineered dataset:  
```bash
uv run -m models.evaluate_models
```  
This step is used to benchmark different models and select the best-performing one.

---

## JupyterLab

JupyterLab can be launched locally with:  
```bash
uv run jupyter lab
```  
Make sure to use the project virtual environment as the kernel.

---

## Streamlit application

### Local execution

```bash
uv run streamlit run app/kurkkumopo_app.py
```

---

## Notes and limitations

- Predictions are probabilistic, not guarantees.  
- Results are based on historical Kickstarter data.  
- Model performance depends on feature quality and data coverage.  
